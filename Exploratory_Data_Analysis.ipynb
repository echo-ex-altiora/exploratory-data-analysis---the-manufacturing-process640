{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 : Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_utils import RDSDatabaseConnector\n",
    "from EDA_classes import DataTransform\n",
    "from EDA_classes import DataFrameInfo\n",
    "from EDA_classes import Plotter\n",
    "from EDA_classes import DataframeTransform\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency # type: ignore\n",
    "from scipy.stats import normaltest # type: ignore\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from scipy.stats import linregress # type: ignore\n",
    "\n",
    "connect = RDSDatabaseConnector()\n",
    "failure_df = connect.extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 : Convert Columns to correct format\n",
    "Now that you have familiarised yourself with the data we want to alter any columns that aren't in the correct format. Are there columns that could be represented better numerically, are dates in the correct format, should some columns be categorical? Are there any excess symbols in the data?\n",
    "\n",
    "If there are columns which you think should be converted into a different format, create a DataTransform class to handle these conversions. Within the DataTransform class add methods which you can apply to your DataFrame columns to perform any required conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_dataset = DataTransform(failure_df)\n",
    "transform_dataset.auto_to_boolean()\n",
    "transform_dataset.manual_to_categorical('Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 : Create DataframeInfo class\n",
    "\n",
    "After converting your columns to a more appropriate format you may want to develop a class to extract information from the DataFrame and its columns. Create a DataFrameInfo class which will contain methods that generate useful information about your DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful utility methods you might want to create that are often used for EDA tasks are:\n",
    "\n",
    "- Describe all columns in the DataFrame to check their data types\n",
    "- Extract statistical values: median, standard deviation and mean from the columns and the DataFrame\n",
    "- Count distinct values in categorical columns\n",
    "- Print out the shape of the DataFrame\n",
    "- Generate a count/percentage count of NULL values in each column\n",
    "- Any other methods you may find useful\n",
    "Creating a class like this will be useful in the future since these are common tasks for performing EDA on any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = DataFrameInfo(failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 : Remove/impute missing values in the data\n",
    "An important EDA task is to impute or remove missing values from the dataset. Missing values can occur due to a variety of reasons such as data entry errors or incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this task, you will first identify the variables with missing values and determine the percentage of missing values in each variable. Depending on the extent of missing data, you may choose to either impute the missing values or remove them from the dataset.\n",
    "\n",
    "If the percentage of missing data is relatively small, you may choose to impute the missing values using statistical methods such as mean or median imputation. Alternatively, if the percentage of missing data is large or if the missing data is not missing at random, you may choose to remove the variables or rows with missing data entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Create Plotter and DataTransform Classes (find in EDA_classes)\n",
    "- A Plotter class to visualise insights from the data\n",
    "- A DataFrameTransformclass to perform EDA transformations on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Plotter(failure_df)\n",
    "transform = DataframeTransform(failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Use a method/function to determine the amount of NULLs in each column. Determine which columns should be dropped and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.percentage_null()\n",
    "print(info.find_columns_with_missing_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three columns with missing values are Tool wear, Air temp and Process temp which are all numerical columns\n",
    "The percentage of missing values does not exceed 10% in any of the columns so there is no reason to drop any columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Within your DataFrameTransform class create a method which can impute your DataFrame columns. \n",
    "Decide whether the column should be imputed with the median or the mean and impute the NULL values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to determine if the missing data is missing at random or not missing at random as this will determine whether we choose to remove rows or impute data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets get an overview of the correlation between the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.correlation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap we can see that:\n",
    "- Air Temperature and Process Temperature are strongly correlated as expected\n",
    "- Tool wear doesn't have strong correlation with any other values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to having no significant correlation, we can probably risk concluding that Tool wear is missing at random and that we can impute the missing values.\n",
    "To decide what to impute the missing value with, lets look at the distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.kde_plot('Tool wear [min]')\n",
    "info.manual_extract_stats('Tool wear [min]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution looks fairly flat and the mean and median are very similar so we could impute with either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.impute_with_mean('Tool wear [min]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets further investigate the correlation between process and air temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.scatter_plot('Air temperature [K]', 'Process temperature [K]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential approach we could take, since they are strongly correlated, is to use the mean of the difference to impute the missing values\n",
    "\n",
    "This method would require us to drop rows where both air and process temp are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ['Air temperature [K]', 'Process temperature [K]']\n",
    "temp_df = failure_df[temp]\n",
    "temp_df['difference'] = temp_df['Process temperature [K]'] - temp_df['Air temperature [K]']\n",
    "temp_info = DataFrameInfo(temp_df)\n",
    "temp_info.extract_stats()\n",
    "temp_df = temp_df.dropna(subset=['Process temperature [K]','Air temperature [K]'], how='all')\n",
    "temp_df['Process temperature [K]'] = temp_df['Process temperature [K]'].fillna(temp_df['Air temperature [K]'] + temp_df['difference'].mean())\n",
    "temp_df['Air temperature [K]'] = temp_df['Air temperature [K]'].fillna(temp_df['Process temperature [K]'] - temp_df['difference'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively...\n",
    "\n",
    "Use the D’Agostino’s K^2 Test (goodness-of-fit measure of departure from normality) to test for normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = failure_df['Air temperature [K]']\n",
    "info.manual_extract_stats('Air temperature [K]')\n",
    "plot.kde_plot('Air temperature [K]')\n",
    "# D’Agostino’s K^2 Test\n",
    "stat, p = normaltest(data, nan_policy='omit')\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Air temperature looks to be normally distributed and the median and mean are very similar so we can impute with either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.impute_with_mean('Air temperature [K]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = failure_df['Process temperature [K]']\n",
    "info.manual_extract_stats('Process temperature [K]')\n",
    "plot.kde_plot('Process temperature [K]') \n",
    "# D’Agostino’s K^2 Test\n",
    "stat, p = normaltest(data, nan_policy='omit')\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process temperature looks normally distributed and the median and mean are very similar so we can impute with either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.impute_with_mean('Process temperature [K]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Run your NULL checking method/function again to check that all NULLs have been removed. \n",
    "Generate a plot by creating a method in your Plotter class to visualise the removal of NULL values. ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.percentage_null()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 : Perform transformations on skewed columns\n",
    "Skewed data can lead to biased models and inaccurate results, so it's important to address this issue before proceeding with any analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Firstly you will need to identify the skewed columns in the data. \n",
    "This can be done using standard Pandas methods. You then need to determine a threshold for the skewness of the data, over which a column will be considered skewed. You should also visualise the data using your Plotter class to analyse the skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(failure_df.columns)\n",
    "numeric_features = [col for col in failure_df.columns[1:]\n",
    "                    if failure_df[col].dtype == 'float64' or failure_df[col].dtype == 'int64']\n",
    "\n",
    "columns_with_high_skew = []\n",
    "for column in numeric_features:\n",
    "    print(f\"Skew of {column} column is {failure_df[column].skew()}\")\n",
    "    if failure_df[column].skew() > 1 or failure_df[column].skew() < -1:\n",
    "        plot.histogram(column)\n",
    "        plot.qq_plot(column)\n",
    "        columns_with_high_skew.append(column)\n",
    "\n",
    "print(columns_with_high_skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the histograms of the different columns, I've decided that the threshold for skewness should be 1/-1\n",
    "\n",
    "The main contender for skew is Rotational speed which has a big gap between the 3rd quartile and the max\n",
    "\n",
    "Looking at the histogram and qq-plot we can see is heavily right skewed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Once the skewed columns are identified, you should perform transformations on these columns to determine which transformation results in the biggest reduction in skew. \n",
    "Create the the method to transform the columns in your DateFrameTransform class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.skew_log(\"Rotational speed [rpm]\")\n",
    "plot.skew_boxcox(\"Rotational speed [rpm]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Apply the identified transformations to the columns to reduce their skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform.correct_skew_log(\"Rotational speed [rpm]\")\n",
    "transform.correct_skew_boxcox(\"Rotational speed [rpm]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Visualise the data to check the results of the transformation have improved the skewness of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_with_high_skew:\n",
    "    print(f\"Skew of {column} column is {failure_df[column].skew()}\")\n",
    "    plot.histogram(column)\n",
    "    plot.qq_plot(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 : At this point you may want to save a separate copy of your DataFrame to compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect.save_edited_dataframe(failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 : Remove outliers from the data\n",
    "\n",
    "Removing outliers from the dataset will improve the quality and accuracy of the analysis as outliers can distort the analysis results. You will need to first identify the outliers and then use a method to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : First visualise your data using your Plotter class to determine if the columns contain outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual methods for detecting outliers are box plots and histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.box_plot('Air temperature [K]') # zero outliers\n",
    "plot.histogram('Air temperature [K]', 100) # zero outliers\n",
    "plot.box_plot('Process temperature [K]')\n",
    "plot.histogram('Process temperature [K]', 100)\n",
    "plot.box_plot('Rotational speed [rpm]')\n",
    "plot.histogram('Rotational speed [rpm]', 100)\n",
    "plot.box_plot('Torque [Nm]')\n",
    "plot.histogram('Torque [Nm]', 100)\n",
    "plot.box_plot('Tool wear [min]')\n",
    "plot.histogram('Tool wear [min]', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From initial visual inspection, we can determine that:\n",
    "- Air temperature [K] and Tool wear [min] have 0 potential outliers \n",
    "- Process temperature [K] may have a couple on the lower end but they are only just outside the whiskers of the boxplot\n",
    "- Rotation speed [rpm] and Torque [Nm] both seem to have quite alot of potential outliers on the boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical methods are z-score and Interquartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_features:\n",
    "    mean = failure_df[column].mean()\n",
    "    std = failure_df[column].std()\n",
    "    print(f'Mean of {column} is {mean} and standard deviation is {std}')\n",
    "    z_scores = (failure_df[column] - mean) / std\n",
    "    failure_df_z_scores = failure_df\n",
    "    failure_df_z_scores['z_scores'] = z_scores\n",
    "    subset = failure_df_z_scores.loc[failure_df_z_scores['z_scores'] > 3, [column, 'z_scores']]\n",
    "    if len(subset) == 0:\n",
    "        print(f'{column} has 0 potential outliers \\n')\n",
    "    else:\n",
    "        print(subset)\n",
    "        print(f'Number of potential outliers is : {len(subset)} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Z-Score is a statistical measurement that describes a data point's position relative to the mean of a group of values measured in terms of standard deviations. \n",
    "\n",
    "Data points with a Z-Score having a high absolute value, typically beyond a threshold like 2 or 3, are often considered outliers as they significantly deviate from the average\n",
    "\n",
    "Summary: Using a theshold of 3, Torque has 14 potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_features:\n",
    "    Q1 = failure_df[column].quantile(0.25)\n",
    "    Q3 = failure_df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    print(f\"Q1 (25th percentile): {Q1}\")\n",
    "    print(f\"Q3 (75th percentile): {Q3}\")\n",
    "    print(f\"IQR: {IQR}\")\n",
    "    outliers = failure_df[(failure_df[column] < (Q1 - 2 * IQR)) | (failure_df[column] > (Q3 + 2 * IQR))]\n",
    "    if len(outliers) > 0:\n",
    "        print(f'The number of potential outliers : {len(outliers)}')\n",
    "        print(f\"Outliers of {column} column:\")\n",
    "        print(outliers[column])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print(f'{column} has 0 outliers \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values that fall below the first quartile minus 1.5 times the IQR or above the third quartile plus 1.5 times the IQR are typically classified as outliers, as they lie outside the common range of variability in the data.\n",
    "\n",
    "Summary:\n",
    "- Process temp has 10 potential outliers\n",
    "- Rotational speed has 90 potential outliers\n",
    "- Torque has 69 potential outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Once identified use a method to transform or remove the outliers from the dataset. Build this method in your DataFrameTransform class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.remove_outliers_IQR('Rotational speed [rpm]', 1.5)\n",
    "transform.remove_outliers_IQR('Torque [Nm]', 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : With the outliers transformed/removed re-visualise your data with you Plotter class to check that the outliers have been correctly removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.box_plot('Rotational speed [rpm]')\n",
    "plot.histogram('Rotational speed [rpm]', 100)\n",
    "\n",
    "plot.box_plot('Torque [Nm]')\n",
    "plot.histogram('Torque [Nm]', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to check for correlation between columns investigate any data points that deviate significantly from the general trend, indicating potential outliers.\n",
    "\n",
    "Visual methods for detecting outliers re scatter plots and regression lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.pair_plot()\n",
    "plot.correlation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong positive correlation between Air temeprature and Process temperature and a strong negative correlation between Rotational speed and Torque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.scatter_plot('Rotational speed [rpm]', 'Torque [Nm]')\n",
    "plot.scatter_with_lin_regression('Rotational speed [rpm]', 'Torque [Nm]')\n",
    "\n",
    "plot.scatter_plot('Air temperature [K]', 'Process temperature [K]')\n",
    "plot.scatter_with_lin_regression('Air temperature [K]', 'Process temperature [K]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are significant amount of ouliers in the scatterplot for Air temperature against Process temperature due to filling in null values..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 : Dropping overly correlated columns\n",
    "\n",
    "Highly correlated columns in a dataset can lead to multicollinearity issues, which can affect the accuracy and interpretability of models built on the data. In this task, you will identify highly correlated columns and remove them to improve the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : First compute the correlation matrix for the dataset and visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.correlation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Identify which columns are highly correlated. You will need to decide on a correlation threshold and to remove all columns above this threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong positive correlation between Air and Process temp and a strong negative correlation between Rotational speed and Torque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_high_correlation = []\n",
    "for column_1 in numeric_features:\n",
    "    for column_2 in numeric_features:\n",
    "        if column_1 == column_2:\n",
    "            continue\n",
    "        else:\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(failure_df[column_1], failure_df[column_2])\n",
    "            if round(r_value, 1) >= 0.8:\n",
    "                columns_with_high_correlation.append(column_1)\n",
    "print(columns_with_high_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Decide which columns can be removed based on the results of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove either Air or Process temp in this case but since Process temperature is the temperature the machine was operating at during production while air temperature is just the average room temperature, we should keep Process temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Remove the highly correlated columns from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_df.drop('Air temperature [K]', axis=1, inplace=True)\n",
    "print(failure_df.head(5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
